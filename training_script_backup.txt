cell 1
# Install required packages
import sys

# Check Python version
print(f"üêç Python version: {sys.version}")
python_version = sys.version_info

# Since MediaPipe doesn't work with Python 3.11+, we'll use YOLOv8-Pose instead
if python_version.major == 3 and python_version.minor >= 11:
    print("‚úÖ Using Python 3.11+ - Installing YOLOv8-Pose")
    !pip install ultralytics --quiet  # YOLOv8 with pose estimation
else:
    print("‚úÖ Using Python 3.10 or earlier - Installing MediaPipe")
    !pip install mediapipe --quiet

# Install other packages
!pip install opencv-python-headless --quiet  
!pip install torch torchvision torchaudio --quiet
!pip install scikit-learn matplotlib seaborn tqdm --quiet

print("\n‚úÖ All packages installed successfully!")

# Verify installation
print("\nVerifying pose estimation library...")
if python_version.major == 3 and python_version.minor >= 11:
    try:
        from ultralytics import YOLO
        print("‚úÖ YOLOv8 (with pose estimation) installed successfully")
        print("‚úÖ Will use YOLOv8-Pose for skeleton extraction")
    except Exception as e:
        print(f"‚ùå YOLOv8 import error: {e}")
else:
    try:
        import mediapipe as mp
        print(f"‚úÖ MediaPipe version: {mp.__version__}")
        print(f"‚úÖ MediaPipe solutions available: {hasattr(mp, 'solutions')}")
    except Exception as e:
        print(f"‚ùå MediaPipe import error: {e}")

cell 2
# Import libraries
import os
import sys
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from tqdm.notebook import tqdm
import json
import time
import warnings
warnings.filterwarnings('ignore')

# Conditional import: MediaPipe for Python 3.10, YOLOv8 for Python 3.11+
python_version = sys.version_info
USE_YOLO_POSE = python_version.major == 3 and python_version.minor >= 11

if not USE_YOLO_POSE:
    # MediaPipe for Python 3.10 and below
    import mediapipe as mp
    print("üì¶ Imported MediaPipe for pose estimation")
else:
    # YOLOv8 will be imported later in the SkeletonExtractor class
    print("üì¶ Will use YOLOv8-Pose for pose estimation (Python 3.11+)")

# PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

# Scikit-learn
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score
from sklearn.model_selection import train_test_split

# Set random seeds for reproducibility
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üöÄ Using device: {device}")
print(f"PyTorch version: {torch.__version__}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")


cell 3
# Configuration
CONFIG = {
    # Dataset paths (Kaggle) - FIXED: Added RWF-2000 nested folder
    'DATASET_PATH': '/kaggle/input/rwf2000/RWF-2000',  # Note the nested RWF-2000 folder!
    'TRAIN_PATH': '/kaggle/input/rwf2000/RWF-2000/train',
    'VAL_PATH': '/kaggle/input/rwf2000/RWF-2000/val',
    
    # Alternative: Local paths (uncomment if using local dataset)
    # 'DATASET_PATH': '../dataset/rwf2000',
    # 'TRAIN_PATH': '../dataset/rwf2000/train',
    # 'VAL_PATH': '../dataset/rwf2000/val',
    
    # Pose extraction
    'NUM_KEYPOINTS': 33,  # MediaPipe Pose landmarks
    'NUM_COORDS': 3,      # (x, y, visibility)
    'SEQUENCE_LENGTH': 16, # Number of frames per video clip
    'FRAME_STRIDE': 2,     # Skip frames for efficiency
    
    # Model hyperparameters
    'HIDDEN_SIZE': 256,
    'NUM_LAYERS': 2,
    'DROPOUT': 0.3,
    'NUM_CLASSES': 2,
    
    # Training hyperparameters
    'BATCH_SIZE': 32,
    'LEARNING_RATE': 1e-3,
    'EPOCHS': 30,
    'WEIGHT_DECAY': 1e-5,
    'EARLY_STOPPING_PATIENCE': 7,
    
    # Output
    'OUTPUT_DIR': './skeletal_violence_model',
    'MODEL_SAVE_PATH': './skeletal_violence_model/best_skeleton_model.pth',
    'RESULTS_PATH': './skeletal_violence_model/training_results.json'
}

# Create output directory
os.makedirs(CONFIG['OUTPUT_DIR'], exist_ok=True)

print("üìã Configuration loaded:")
for key, value in CONFIG.items():
    print(f"  {key}: {value}")

cell 4
# First, let's explore the actual directory structure
print("üîç Exploring actual dataset structure...\n")

def explore_directory(path, max_depth=3, current_depth=0, prefix=""):
    """Recursively explore directory structure"""
    if current_depth >= max_depth:
        return
    
    try:
        items = os.listdir(path)
        for item in sorted(items)[:20]:  # Limit to first 20 items
            item_path = os.path.join(path, item)
            if os.path.isdir(item_path):
                print(f"{prefix}üìÅ {item}/")
                explore_directory(item_path, max_depth, current_depth + 1, prefix + "  ")
            else:
                # Show file extension and count
                print(f"{prefix}üìÑ {item}")
    except PermissionError:
        print(f"{prefix}‚ö†Ô∏è Permission denied")

# Explore the dataset directory
if os.path.exists(CONFIG['DATASET_PATH']):
    explore_directory(CONFIG['DATASET_PATH'], max_depth=4)
    
    # Also check what's directly in the path
    print("\n" + "="*50)
    print("Direct contents of dataset path:")
    try:
        contents = os.listdir(CONFIG['DATASET_PATH'])
        print(f"Found {len(contents)} items: {contents}")
    except:
        print("Could not list contents")
else:
    print(f"‚ùå Path does not exist: {CONFIG['DATASET_PATH']}")

cell 5
# Find video files recursively to understand the structure
print("üîé Searching for video files...\n")

def find_video_files(root_path, extensions=['.avi', '.mp4', '.mov', '.mkv']):
    """Find all video files recursively"""
    video_files = []
    
    for dirpath, dirnames, filenames in os.walk(root_path):
        for filename in filenames:
            if any(filename.lower().endswith(ext) for ext in extensions):
                full_path = os.path.join(dirpath, filename)
                rel_path = os.path.relpath(full_path, root_path)
                video_files.append(rel_path)
    
    return video_files

if os.path.exists(CONFIG['DATASET_PATH']):
    videos = find_video_files(CONFIG['DATASET_PATH'])
    
    print(f"Found {len(videos)} video files")
    
    if videos:
        print("\nFirst 10 video paths:")
        for video in videos[:10]:
            print(f"  {video}")
        
        # Analyze the structure
        print("\n" + "="*50)
        print("Analyzing structure...")
        
        # Check for common patterns
        folders = set()
        for video in videos:
            parts = video.split(os.sep)
            if len(parts) > 1:
                folders.add(parts[0])
        
        print(f"Top-level folders containing videos: {sorted(folders)}")
        
        # Detect actual paths
        if 'train' in folders or 'val' in folders:
            print("\n‚úÖ Found train/val structure")
        
        # Check for Fight/NonFight or fight/nonfight
        categories = set()
        for video in videos:
            parts = video.split(os.sep)
            for part in parts:
                if 'fight' in part.lower():
                    categories.add(part)
        
        print(f"Categories found: {sorted(categories)}")
else:
    print("Dataset path not found")


cell 6
# Auto-detect and update the correct paths
print("üîß Auto-detecting correct dataset structure...\n")

def detect_rwf2000_structure(base_path):
    """Automatically detect RWF-2000 dataset structure"""
    
    detected_config = {
        'DATASET_PATH': base_path,
        'TRAIN_PATH': None,
        'VAL_PATH': None,
        'video_extension': '.avi',
        'class_names': []
    }
    
    # Search for train and val folders
    for root, dirs, files in os.walk(base_path):
        # Check for train folder
        if 'train' in [d.lower() for d in dirs]:
            train_idx = [d.lower() for d in dirs].index('train')
            detected_config['TRAIN_PATH'] = os.path.join(root, dirs[train_idx])
        
        # Check for val folder
        if 'val' in [d.lower() for d in dirs]:
            val_idx = [d.lower() for d in dirs].index('val')
            detected_config['VAL_PATH'] = os.path.join(root, dirs[val_idx])
        
        # If we found both, no need to search deeper
        if detected_config['TRAIN_PATH'] and detected_config['VAL_PATH']:
            break
    
    # Detect video extension
    for root, dirs, files in os.walk(base_path):
        video_files = [f for f in files if f.endswith(('.avi', '.mp4', '.mov'))]
        if video_files:
            detected_config['video_extension'] = os.path.splitext(video_files[0])[1]
            break
    
    # Detect class names
    if detected_config['TRAIN_PATH']:
        try:
            class_folders = [d for d in os.listdir(detected_config['TRAIN_PATH']) 
                           if os.path.isdir(os.path.join(detected_config['TRAIN_PATH'], d))]
            detected_config['class_names'] = class_folders
        except:
            pass
    
    return detected_config

if os.path.exists(CONFIG['DATASET_PATH']):
    detected = detect_rwf2000_structure(CONFIG['DATASET_PATH'])
    
    print("Detected configuration:")
    print("=" * 60)
    for key, value in detected.items():
        print(f"  {key}: {value}")
    print("=" * 60)
    
    # Update CONFIG if paths were found
    if detected['TRAIN_PATH']:
        CONFIG['TRAIN_PATH'] = detected['TRAIN_PATH']
        print(f"\n‚úÖ Updated TRAIN_PATH: {CONFIG['TRAIN_PATH']}")
    
    if detected['VAL_PATH']:
        CONFIG['VAL_PATH'] = detected['VAL_PATH']
        print(f"‚úÖ Updated VAL_PATH: {CONFIG['VAL_PATH']}")
    
    # Show what's in these folders
    if detected['TRAIN_PATH'] and os.path.exists(detected['TRAIN_PATH']):
        print(f"\nüìÇ Contents of train folder:")
        print(f"   {os.listdir(detected['TRAIN_PATH'])}")
    
    if detected['VAL_PATH'] and os.path.exists(detected['VAL_PATH']):
        print(f"\nüìÇ Contents of val folder:")
        print(f"   {os.listdir(detected['VAL_PATH'])}")
        
else:
    print("‚ùå Dataset path does not exist")


cell 7
# Updated explore function with flexible video extension and class name detection
def explore_dataset(base_path):
    """Explore RWF-2000 dataset structure with flexible detection"""
    
    stats = {
        'train': {},
        'val': {}
    }
    
    for split in ['train', 'val']:
        split_path = os.path.join(base_path, split)
        
        # Try case-insensitive search
        if not os.path.exists(split_path):
            # Try to find the folder case-insensitively
            try:
                parent = os.path.dirname(split_path) or base_path
                folders = os.listdir(parent)
                for folder in folders:
                    if folder.lower() == split.lower():
                        split_path = os.path.join(parent, folder)
                        break
            except:
                pass
        
        if os.path.exists(split_path):
            # Try common class names (case-insensitive)
            class_variants = [
                ['Fight', 'NonFight'],
                ['fight', 'nonfight'],
                ['Fight', 'Non-Fight'],
                ['violence', 'non-violence'],
                ['Violence', 'NonViolence']
            ]
            
            # Detect actual class folders
            try:
                actual_folders = [d for d in os.listdir(split_path) 
                                if os.path.isdir(os.path.join(split_path, d))]
                
                for folder in actual_folders:
                    folder_path = os.path.join(split_path, folder)
                    # Count videos with various extensions
                    videos = [f for f in os.listdir(folder_path) 
                            if f.endswith(('.avi', '.mp4', '.mov', '.mkv'))]
                    stats[split][folder] = len(videos)
            except Exception as e:
                print(f"Error reading {split_path}: {e}")
    
    return stats

# Check if dataset exists
if os.path.exists(CONFIG['DATASET_PATH']):
    print(f"‚úÖ Dataset found at: {CONFIG['DATASET_PATH']}\n")
    
    stats = explore_dataset(CONFIG['DATASET_PATH'])
    
    print("üìä Dataset Statistics:")
    print("=" * 50)
    print(f"TRAIN SET:")
    train_total = 0
    for category, count in stats['train'].items():
        print(f"  {category} videos: {count}")
        train_total += count
    print(f"  Total: {train_total}")
    
    print(f"\nVALIDATION SET:")
    val_total = 0
    for category, count in stats['val'].items():
        print(f"  {category} videos: {count}")
        val_total += count
    print(f"  Total: {val_total}")
    
    print(f"\nGRAND TOTAL: {train_total + val_total} videos")
    print("=" * 50)
    
    if train_total == 0 and val_total == 0:
        print("\n‚ö†Ô∏è No videos found! Run the cells above to diagnose the structure.")
    
else:
    print(f"‚ö†Ô∏è Dataset not found at: {CONFIG['DATASET_PATH']}")
    print("Please ensure the RWF-2000 dataset is added to your Kaggle notebook.")
    print("Add it via: Data ‚Üí Add Data ‚Üí Search 'rwf-2000'")


cell 8
# YOLOv8-Pose Skeleton Extractor
print("üöÄ Initializing YOLOv8-Pose skeleton extractor...")

from ultralytics import YOLO

class SkeletonExtractor:
    """Extract skeletal keypoints using YOLOv8-Pose"""
    
    def __init__(self):
        # Load YOLOv8 Pose model (will download on first use, ~6MB)
        # Silent loading - no prints to avoid cluttering output during training
        self.model = YOLO('yolov8n-pose.pt')
    
    def extract_keypoints(self, frame):
        """Extract 17 COCO keypoints from a frame"""
        results = self.model(frame, verbose=False)
        
        if len(results) > 0 and results[0].keypoints is not None:
            keypoints = results[0].keypoints.xy.cpu().numpy()
            if len(keypoints) > 0:
                # Get first person's keypoints (17 keypoints, x,y coords)
                kp = keypoints[0].flatten()  # Shape: (34,) for 17 keypoints
                
                # Add confidence scores as visibility
                conf = results[0].keypoints.conf.cpu().numpy()[0] if results[0].keypoints.conf is not None else np.ones(17)
                
                # Create full feature vector: [x1,y1,conf1, x2,y2,conf2, ...]
                full_kp = np.zeros(17 * 3, dtype=np.float32)
                for i in range(17):
                    if i * 2 + 1 < len(kp):
                        full_kp[i*3] = kp[i*2] / 640.0      # Normalize x
                        full_kp[i*3 + 1] = kp[i*2 + 1] / 640.0  # Normalize y
                        full_kp[i*3 + 2] = conf[i] if i < len(conf) else 0.0
                
                # Pad to 99 to match expected input size (33 keypoints * 3)
                padded = np.zeros(99, dtype=np.float32)
                padded[:51] = full_kp
                return padded
        
        return np.zeros(99, dtype=np.float32)
    
    def extract_from_video(self, video_path, num_frames=16, stride=2):
        """Extract skeleton sequence from video"""
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            return None
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        if total_frames < num_frames * stride:
            frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
        else:
            start_idx = (total_frames - num_frames * stride) // 2
            frame_indices = np.arange(start_idx, start_idx + num_frames * stride, stride)
        
        skeletons = []
        
        for frame_idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            
            if ret:
                skeleton = self.extract_keypoints(frame)
                skeletons.append(skeleton)
            else:
                skeletons.append(np.zeros(99, dtype=np.float32))
        
        cap.release()
        return np.array(skeletons, dtype=np.float32)

print("‚úÖ SkeletonExtractor class defined!")


cell 9
class SkeletonViolenceDataset(Dataset):
    """PyTorch Dataset for Skeletal Violence Detection"""
    
    def __init__(self, data_path, sequence_length=16, stride=2, transform=None, cache_dir=None):
        """
        Args:
            data_path: Path to train/val folder containing Fight and NonFight folders
            sequence_length: Number of frames per sequence
            stride: Frame sampling stride
            transform: Optional data augmentation
            cache_dir: Directory to cache extracted skeletons (for faster loading)
        """
        self.data_path = data_path
        self.sequence_length = sequence_length
        self.stride = stride
        self.transform = transform
        self.cache_dir = cache_dir
        
        # Create cache directory if specified
        if self.cache_dir:
            os.makedirs(self.cache_dir, exist_ok=True)
        
        # Load video paths and labels
        self.video_paths = []
        self.labels = []
        
        # Class mapping: NonFight=0, Fight=1
        class_mapping = {'NonFight': 0, 'Fight': 1}
        
        for class_name, label in class_mapping.items():
            class_path = os.path.join(data_path, class_name)
            if os.path.exists(class_path):
                videos = [f for f in os.listdir(class_path) if f.endswith('.avi')]
                for video in videos:
                    self.video_paths.append(os.path.join(class_path, video))
                    self.labels.append(label)
        
        print(f"‚úÖ Loaded {len(self.video_paths)} videos from {data_path}")
        print(f"   NonFight: {sum(1 for l in self.labels if l == 0)}")
        print(f"   Fight: {sum(1 for l in self.labels if l == 1)}")
    
    def __len__(self):
        return len(self.video_paths)
    
    def _get_cache_path(self, idx):
        """Get cache file path for a video"""
        if self.cache_dir is None:
            return None
        video_name = os.path.basename(self.video_paths[idx]).replace('.avi', '.npy')
        return os.path.join(self.cache_dir, video_name)
    
    def __getitem__(self, idx):
        """Get skeleton sequence and label"""
        
        # Check cache first
        cache_path = self._get_cache_path(idx)
        if cache_path and os.path.exists(cache_path):
            skeleton_sequence = np.load(cache_path)
        else:
            # Extract skeleton from video
            extractor = SkeletonExtractor()
            skeleton_sequence = extractor.extract_from_video(
                self.video_paths[idx],
                num_frames=self.sequence_length,
                stride=self.stride
            )
            
            # Save to cache
            if cache_path and skeleton_sequence is not None:
                np.save(cache_path, skeleton_sequence)
        
        # Handle extraction failure
        if skeleton_sequence is None:
            skeleton_sequence = np.zeros(
                (self.sequence_length, CONFIG['NUM_KEYPOINTS'] * CONFIG['NUM_COORDS']),
                dtype=np.float32
            )
        
        # Apply transform if any
        if self.transform:
            skeleton_sequence = self.transform(skeleton_sequence)
        
        # Convert to tensor
        skeleton_tensor = torch.tensor(skeleton_sequence, dtype=torch.float32)
        label = torch.tensor(self.labels[idx], dtype=torch.long)
        
        return skeleton_tensor, label

print("‚úÖ Dataset class defined")


cell 10
class SkeletonViolenceDataset(Dataset):
    """PyTorch Dataset for Skeletal Violence Detection"""
    
    def __init__(self, data_path, sequence_length=16, stride=2, transform=None, cache_dir=None):
        """
        Args:
            data_path: Path to train/val folder containing Fight and NonFight folders
            sequence_length: Number of frames per sequence
            stride: Frame sampling stride
            transform: Optional data augmentation
            cache_dir: Directory to cache extracted skeletons (for faster loading)
        """
        self.data_path = data_path
        self.sequence_length = sequence_length
        self.stride = stride
        self.transform = transform
        self.cache_dir = cache_dir
        
        # Create cache directory if specified
        if self.cache_dir:
            os.makedirs(self.cache_dir, exist_ok=True)
        
        # Load video paths and labels
        self.video_paths = []
        self.labels = []
        
        # Class mapping: NonFight=0, Fight=1
        class_mapping = {'NonFight': 0, 'Fight': 1}
        
        for class_name, label in class_mapping.items():
            class_path = os.path.join(data_path, class_name)
            if os.path.exists(class_path):
                videos = [f for f in os.listdir(class_path) if f.endswith('.avi')]
                for video in videos:
                    self.video_paths.append(os.path.join(class_path, video))
                    self.labels.append(label)
        
        print(f"‚úÖ Loaded {len(self.video_paths)} videos from {data_path}")
        print(f"   NonFight: {sum(1 for l in self.labels if l == 0)}")
        print(f"   Fight: {sum(1 for l in self.labels if l == 1)}")
    
    def __len__(self):
        return len(self.video_paths)
    
    def _get_cache_path(self, idx):
        """Get cache file path for a video"""
        if self.cache_dir is None:
            return None
        video_name = os.path.basename(self.video_paths[idx]).replace('.avi', '.npy')
        return os.path.join(self.cache_dir, video_name)
    
    def __getitem__(self, idx):
        """Get skeleton sequence and label"""
        
        # Check cache first
        cache_path = self._get_cache_path(idx)
        if cache_path and os.path.exists(cache_path):
            skeleton_sequence = np.load(cache_path)
        else:
            # Extract skeleton from video
            extractor = SkeletonExtractor()
            skeleton_sequence = extractor.extract_from_video(
                self.video_paths[idx],
                num_frames=self.sequence_length,
                stride=self.stride
            )
            
            # Save to cache
            if cache_path and skeleton_sequence is not None:
                np.save(cache_path, skeleton_sequence)
        
        # Handle extraction failure
        if skeleton_sequence is None:
            skeleton_sequence = np.zeros(
                (self.sequence_length, CONFIG['NUM_KEYPOINTS'] * CONFIG['NUM_COORDS']),
                dtype=np.float32
            )
        
        # Apply transform if any
        if self.transform:
            skeleton_sequence = self.transform(skeleton_sequence)
        
        # Convert to tensor
        skeleton_tensor = torch.tensor(skeleton_sequence, dtype=torch.float32)
        label = torch.tensor(self.labels[idx], dtype=torch.long)
        
        return skeleton_tensor, label

print("‚úÖ Dataset class defined")


cell 11
class SkeletonViolenceModel(nn.Module):
    """Skeleton-based Violence Detection Model with LSTM"""
    
    def __init__(self, num_keypoints=33, num_coords=3, hidden_size=256, num_layers=2, 
                 num_classes=2, dropout=0.3):
        """
        Args:
            num_keypoints: Number of skeletal keypoints (33 for MediaPipe)
            num_coords: Number of coordinates per keypoint (x, y, visibility = 3)
            hidden_size: LSTM hidden size
            num_layers: Number of LSTM layers
            num_classes: Number of output classes (2: Violence/Non-Violence)
            dropout: Dropout rate
        """
        super(SkeletonViolenceModel, self).__init__()
        
        self.input_size = num_keypoints * num_coords  # 33 * 3 = 99
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # Feature encoder for skeleton
        self.encoder = nn.Sequential(
            nn.Linear(self.input_size, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # LSTM for temporal modeling
        self.lstm = nn.LSTM(
            input_size=256,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size * 2, 128),  # *2 for bidirectional
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, num_classes)
        )
    
    def forward(self, x):
        """
        Args:
            x: Input tensor of shape (batch_size, sequence_length, num_keypoints * num_coords)
            
        Returns:
            Output logits of shape (batch_size, num_classes)
        """
        batch_size, seq_len, _ = x.size()
        
        # Encode each frame's skeleton
        # Reshape: (batch * seq_len, input_size)
        x_flat = x.view(batch_size * seq_len, -1)
        encoded = self.encoder(x_flat)
        
        # Reshape back: (batch, seq_len, 256)
        encoded = encoded.view(batch_size, seq_len, -1)
        
        # LSTM
        lstm_out, (hidden, cell) = self.lstm(encoded)
        
        # Use last time step output
        last_output = lstm_out[:, -1, :]
        
        # Classification
        logits = self.classifier(last_output)
        
        return logits
    
    def get_num_params(self):
        """Get number of trainable parameters"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

# Initialize model
model = SkeletonViolenceModel(
    num_keypoints=CONFIG['NUM_KEYPOINTS'],
    num_coords=CONFIG['NUM_COORDS'],
    hidden_size=CONFIG['HIDDEN_SIZE'],
    num_layers=CONFIG['NUM_LAYERS'],
    num_classes=CONFIG['NUM_CLASSES'],
    dropout=CONFIG['DROPOUT']
).to(device)

print("üèóÔ∏è Model Architecture:")
print("=" * 60)
print(model)
print("=" * 60)
print(f"üìä Total trainable parameters: {model.get_num_params():,}")


cell 12
# Create datasets with caching for faster loading
print("üì¶ Creating datasets...")

# Cache directories for faster loading
train_cache_dir = os.path.join(CONFIG['OUTPUT_DIR'], 'cache_train')
val_cache_dir = os.path.join(CONFIG['OUTPUT_DIR'], 'cache_val')

train_dataset = SkeletonViolenceDataset(
    data_path=CONFIG['TRAIN_PATH'],
    sequence_length=CONFIG['SEQUENCE_LENGTH'],
    stride=CONFIG['FRAME_STRIDE'],
    cache_dir=train_cache_dir
)

val_dataset = SkeletonViolenceDataset(
    data_path=CONFIG['VAL_PATH'],
    sequence_length=CONFIG['SEQUENCE_LENGTH'],
    stride=CONFIG['FRAME_STRIDE'],
    cache_dir=val_cache_dir
)

# Create data loaders
# Note: num_workers=0 to avoid CUDA multiprocessing issues with YOLOv8
train_loader = DataLoader(
    train_dataset,
    batch_size=CONFIG['BATCH_SIZE'],
    shuffle=True,
    num_workers=0,  # Single process to avoid CUDA fork issues
    pin_memory=True if torch.cuda.is_available() else False
)

val_loader = DataLoader(
    val_dataset,
    batch_size=CONFIG['BATCH_SIZE'],
    shuffle=False,
    num_workers=0,  # Single process to avoid CUDA fork issues
    pin_memory=True if torch.cuda.is_available() else False
)

print(f"‚úÖ Data loaders created")
print(f"   Train batches: {len(train_loader)}")
print(f"   Val batches: {len(val_loader)}")


cell 13
# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(
    model.parameters(),
    lr=CONFIG['LEARNING_RATE'],
    weight_decay=CONFIG['WEIGHT_DECAY']
)

# Learning rate scheduler
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='max',
    factor=0.5,
    patience=3
)

# Training history
history = {
    'train_loss': [],
    'train_acc': [],
    'val_loss': [],
    'val_acc': [],
    'val_precision': [],
    'val_recall': [],
    'val_f1': [],
    'learning_rate': []
}

# Best model tracking
best_val_acc = 0.0
best_epoch = 0
patience_counter = 0

print("‚úÖ Training setup complete")


cell 14
def train_one_epoch(model, dataloader, criterion, optimizer, device):
    """Train for one epoch"""
    model.train()
    
    running_loss = 0.0
    all_preds = []
    all_labels = []
    
    pbar = tqdm(dataloader, desc='Training', leave=False)
    
    for skeletons, labels in pbar:
        skeletons = skeletons.to(device)
        labels = labels.to(device)
        
        # Forward pass
        optimizer.zero_grad()
        outputs = model(skeletons)
        loss = criterion(outputs, labels)
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Statistics
        running_loss += loss.item() * skeletons.size(0)
        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
        
        # Update progress bar
        pbar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    epoch_loss = running_loss / len(dataloader.dataset)
    epoch_acc = accuracy_score(all_labels, all_preds) * 100
    
    return epoch_loss, epoch_acc

def validate(model, dataloader, criterion, device):
    """Validate the model"""
    model.eval()
    
    running_loss = 0.0
    all_preds = []
    all_labels = []
    all_probs = []
    
    with torch.no_grad():
        pbar = tqdm(dataloader, desc='Validation', leave=False)
        
        for skeletons, labels in pbar:
            skeletons = skeletons.to(device)
            labels = labels.to(device)
            
            # Forward pass
            outputs = model(skeletons)
            loss = criterion(outputs, labels)
            
            # Get probabilities
            probs = F.softmax(outputs, dim=1)
            
            # Statistics
            running_loss += loss.item() * skeletons.size(0)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy()[:, 1])  # Probability of violence class
            
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    epoch_loss = running_loss / len(dataloader.dataset)
    epoch_acc = accuracy_score(all_labels, all_preds) * 100
    epoch_precision = precision_score(all_labels, all_preds, zero_division=0) * 100
    epoch_recall = recall_score(all_labels, all_preds, zero_division=0) * 100
    epoch_f1 = f1_score(all_labels, all_preds, zero_division=0) * 100
    
    return epoch_loss, epoch_acc, epoch_precision, epoch_recall, epoch_f1, all_labels, all_preds, all_probs

print("‚úÖ Training functions defined")


cell 15
# Training loop
print("üöÄ Starting training...")
print("=" * 70)

start_time = time.time()

for epoch in range(CONFIG['EPOCHS']):
    epoch_start = time.time()
    
    # Train
    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)
    
    # Validate
    val_loss, val_acc, val_precision, val_recall, val_f1, val_labels, val_preds, val_probs = validate(
        model, val_loader, criterion, device
    )
    
    # Update learning rate
    scheduler.step(val_acc)
    current_lr = optimizer.param_groups[0]['lr']
    
    # Save history
    history['train_loss'].append(train_loss)
    history['train_acc'].append(train_acc)
    history['val_loss'].append(val_loss)
    history['val_acc'].append(val_acc)
    history['val_precision'].append(val_precision)
    history['val_recall'].append(val_recall)
    history['val_f1'].append(val_f1)
    history['learning_rate'].append(current_lr)
    
    # Print epoch results
    epoch_time = time.time() - epoch_start
    print(f"\nEpoch [{epoch+1}/{CONFIG['EPOCHS']}] - Time: {epoch_time:.2f}s")
    print(f"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
    print(f"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%")
    print(f"  Val Precision: {val_precision:.2f}% | Val Recall: {val_recall:.2f}% | Val F1: {val_f1:.2f}%")
    print(f"  Learning Rate: {current_lr:.6f}")
    
    # Save best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        best_epoch = epoch + 1
        patience_counter = 0
        
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_acc': val_acc,
            'history': history
        }, CONFIG['MODEL_SAVE_PATH'])
        
        print(f"  ‚úÖ New best model saved! (Val Acc: {val_acc:.2f}%)")
    else:
        patience_counter += 1
        print(f"  ‚è≥ Patience: {patience_counter}/{CONFIG['EARLY_STOPPING_PATIENCE']}")
    
    # Early stopping
    if patience_counter >= CONFIG['EARLY_STOPPING_PATIENCE']:
        print(f"\n‚ö†Ô∏è Early stopping triggered at epoch {epoch+1}")
        break
    
    print("-" * 70)

total_time = time.time() - start_time
print("\n" + "=" * 70)
print(f"‚úÖ Training completed!")
print(f"   Total time: {total_time/60:.2f} minutes")
print(f"   Best epoch: {best_epoch}")
print(f"   Best validation accuracy: {best_val_acc:.2f}%")
print("=" * 70)


cell 16
# Load best model
print("üìÇ Loading best model...")
checkpoint = torch.load(CONFIG['MODEL_SAVE_PATH'])
model.load_state_dict(checkpoint['model_state_dict'])
print(f"‚úÖ Loaded model from epoch {checkpoint['epoch']+1}")

# Final evaluation
val_loss, val_acc, val_precision, val_recall, val_f1, val_labels, val_preds, val_probs = validate(
    model, val_loader, criterion, device
)

# ROC AUC Score
try:
    roc_auc = roc_auc_score(val_labels, val_probs)
except:
    roc_auc = 0.0

# Confusion matrix
cm = confusion_matrix(val_labels, val_preds)

# Classification report
class_names = ['NonFight', 'Fight']
report = classification_report(val_labels, val_preds, target_names=class_names, output_dict=True)

# Print results
print("\n" + "=" * 70)
print("üéØ FINAL EVALUATION RESULTS")
print("=" * 70)
print(f"Accuracy: {val_acc:.2f}%")
print(f"Precision: {val_precision:.2f}%")
print(f"Recall: {val_recall:.2f}%")
print(f"F1-Score: {val_f1:.2f}%")
print(f"ROC AUC: {roc_auc:.4f}")
print("\n" + "=" * 70)
print("\nClassification Report:")
print(classification_report(val_labels, val_preds, target_names=class_names))

# Save results
results = {
    'best_epoch': best_epoch,
    'best_val_acc': best_val_acc,
    'final_metrics': {
        'accuracy': val_acc,
        'precision': val_precision,
        'recall': val_recall,
        'f1_score': val_f1,
        'roc_auc': roc_auc
    },
    'confusion_matrix': cm.tolist(),
    'classification_report': report,
    'history': history,
    'config': CONFIG
}

with open(CONFIG['RESULTS_PATH'], 'w') as f:
    json.dump(results, f, indent=2)

print(f"‚úÖ Results saved to {CONFIG['RESULTS_PATH']}")


cell 17
# Plot training history
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Loss
axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')
axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s')
axes[0, 0].set_title('Training & Validation Loss', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# Accuracy
axes[0, 1].plot(history['train_acc'], label='Train Accuracy', marker='o')
axes[0, 1].plot(history['val_acc'], label='Val Accuracy', marker='s')
axes[0, 1].axhline(y=best_val_acc, color='r', linestyle='--', label=f'Best: {best_val_acc:.2f}%')
axes[0, 1].set_title('Training & Validation Accuracy', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('Accuracy (%)')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# Precision, Recall, F1
axes[1, 0].plot(history['val_precision'], label='Precision', marker='o')
axes[1, 0].plot(history['val_recall'], label='Recall', marker='s')
axes[1, 0].plot(history['val_f1'], label='F1-Score', marker='^')
axes[1, 0].set_title('Validation Metrics', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('Epoch')
axes[1, 0].set_ylabel('Score (%)')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# Learning Rate
axes[1, 1].plot(history['learning_rate'], marker='o', color='green')
axes[1, 1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
axes[1, 1].set_xlabel('Epoch')
axes[1, 1].set_ylabel('Learning Rate')
axes[1, 1].set_yscale('log')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(CONFIG['OUTPUT_DIR'], 'training_history.png'), dpi=150, bbox_inches='tight')
plt.show()

print("‚úÖ Training history plotted")


cell 18
# Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
plt.ylabel('True Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)

# Add accuracy metrics
plt.text(0.5, -0.15, f'Accuracy: {val_acc:.2f}% | Precision: {val_precision:.2f}% | Recall: {val_recall:.2f}% | F1: {val_f1:.2f}%', 
         ha='center', va='top', transform=plt.gca().transAxes, fontsize=11)

plt.tight_layout()
plt.savefig(os.path.join(CONFIG['OUTPUT_DIR'], 'confusion_matrix.png'), dpi=150, bbox_inches='tight')
plt.show()

print("‚úÖ Confusion matrix plotted")


cell 19
# Install required packages
import sys

# Check Python version
print(f"üêç Python version: {sys.version}")
python_version = sys.version_info

# Fix numpy/scipy compatibility issue first
print("\nüì¶ Installing compatible package versions...")
!pip install numpy==1.26.4 --quiet
!pip install scipy==1.11.4 --quiet

# Since MediaPipe doesn't work with Python 3.11+, we'll use YOLOv8-Pose instead
if python_version.major == 3 and python_version.minor >= 11:
    print("‚úÖ Using Python 3.11+ - Installing YOLOv8-Pose (MediaPipe alternative)")
    !pip install ultralytics --quiet  # YOLOv8 with pose estimation
else:
    print("‚úÖ Using Python 3.10 or earlier - Installing MediaPipe")
    !pip install mediapipe --quiet

# Install other packages
!pip install opencv-python-headless --quiet  
!pip install torch torchvision torchaudio --quiet
!pip install scikit-learn matplotlib seaborn tqdm --quiet

print("\n‚úÖ All packages installed successfully!")

# Verify installation
print("\nVerifying pose estimation library...")
if python_version.major == 3 and python_version.minor >= 11:
    try:
        from ultralytics import YOLO
        print("‚úÖ YOLOv8 (with pose estimation) installed successfully")
        print("‚úÖ Will use YOLOv8-Pose for skeleton extraction")
    except Exception as e:
        print(f"‚ùå YOLOv8 import error: {e}")
else:
    try:
        import mediapipe as mp
        print(f"‚úÖ MediaPipe version: {mp.__version__}")
        print(f"‚úÖ MediaPipe solutions available: {hasattr(mp, 'solutions')}")
    except Exception as e:
        print(f"‚ùå MediaPipe import error: {e}")